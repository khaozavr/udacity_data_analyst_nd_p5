{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Fraud in Enron emails and financial data\n",
    "### by Andrii Zakharov\n",
    "\n",
    "In this project, I will try to answer the question whether it is possible to identify Persons of Interest (POIs) in the Enron fraud case by looking at a dataset of Enron employees' emails and their financial data. Even with the data's comparably small size, however, I would not be able to infer much from it by way of manual analysis. Machine learning to the rescue! \n",
    "\n",
    "This dataset is a perfect case for supervised learning classification algorithms, as it contains POI identifiers, and so can be used to both train classifiers and assess their performance. Let's get to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khaoz\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data, I will convert it to a data frame for easier inspection and feature manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, ALLEN PHILLIP K to YEAP SOON\n",
      "Data columns (total 21 columns):\n",
      "salary                       146 non-null object\n",
      "to_messages                  146 non-null object\n",
      "deferral_payments            146 non-null object\n",
      "total_payments               146 non-null object\n",
      "exercised_stock_options      146 non-null object\n",
      "bonus                        146 non-null object\n",
      "restricted_stock             146 non-null object\n",
      "shared_receipt_with_poi      146 non-null object\n",
      "restricted_stock_deferred    146 non-null object\n",
      "total_stock_value            146 non-null object\n",
      "expenses                     146 non-null object\n",
      "loan_advances                146 non-null object\n",
      "from_messages                146 non-null object\n",
      "other                        146 non-null object\n",
      "from_this_person_to_poi      146 non-null object\n",
      "poi                          146 non-null bool\n",
      "director_fees                146 non-null object\n",
      "deferred_income              146 non-null object\n",
      "long_term_incentive          146 non-null object\n",
      "email_address                146 non-null object\n",
      "from_poi_to_this_person      146 non-null object\n",
      "dtypes: bool(1), object(20)\n",
      "memory usage: 24.1+ KB\n",
      "None\n",
      "###############################################\n",
      "We have 146 observations of 21 features, 3066 total data points\n",
      "1358 of those are NaNs, or 44.3 %\n",
      "###############################################\n",
      "Number of POIs: 18 out of 146\n",
      "It's 12.3 %\n"
     ]
    }
   ],
   "source": [
    "# create a data frame from dictionary and inspect it\n",
    "df = pd.DataFrame.from_dict(data_dict, orient=\"index\")\n",
    "print df.info()\n",
    "print \"###############################################\"\n",
    "print \"We have\", df.shape[0], \"observations of\", df.shape[1], \"features,\", df.shape[0]*df.shape[1], \"total data points\"\n",
    "nan_count = 0\n",
    "for feat in df.columns.values:\n",
    "    if 'NaN' in list(df[feat]):\n",
    "        nan_count += df[feat].value_counts()['NaN']\n",
    "print nan_count, \"of those are NaNs, or\", round((float(nan_count) / (df.shape[0]*df.shape[1])) *100, 1), \"%\"\n",
    "print \"###############################################\"\n",
    "print \"Number of POIs:\", sum(df[\"poi\"]), \"out of\", len(df[\"poi\"])\n",
    "print \"It's\", round(df[\"poi\"].mean(), 3) * 100, \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the basic structure of the data. We see feature names, shape, number of NaN values and POIs in our data. Quite a few NaN's there! And only very few POIs, so very imbalanced target classes.\n",
    "\n",
    "One thing that occurs to me is that I can drop the feature \"email_address\", as it will have a unique value for each observation and so won't be able to contribute much to the analysis.\n",
    "\n",
    "Another thing that I remember from the lessons is that the dataset actually contains a row with sum totals for the financial data. Those would definitely be outliers, so I'll drop that row as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop('email_address', axis=1, inplace=True)\n",
    "df.drop(\"TOTAL\", axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably have more outliers in the data, but I would be cautious about simply removing them, as some may contain valuable information about our targets. I will z-transform each feature in the data, define outliers as +-3 SD, and see how many POIs will be found among the outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define outlier searching function\n",
    "from scipy.stats import zscore\n",
    "def is_outlier(feat, thresh=3):\n",
    "    feat_z = zscore(feat)\n",
    "    return abs(feat_z) > thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n outliers in salary 3\n",
      "Prop poi's: 0.667\n",
      "n outliers in to_messages 4\n",
      "Prop poi's: 0.25\n",
      "n outliers in deferral_payments 4\n",
      "Prop poi's: 0.0\n",
      "n outliers in total_payments 1\n",
      "Prop poi's: 1.0\n",
      "n outliers in exercised_stock_options 4\n",
      "Prop poi's: 1.0\n",
      "n outliers in bonus 4\n",
      "Prop poi's: 0.75\n",
      "n outliers in restricted_stock 3\n",
      "Prop poi's: 0.333\n",
      "n outliers in shared_receipt_with_poi 4\n",
      "Prop poi's: 0.25\n",
      "n outliers in restricted_stock_deferred 1\n",
      "Prop poi's: 0.0\n",
      "n outliers in total_stock_value 5\n",
      "Prop poi's: 0.8\n",
      "n outliers in expenses 3\n",
      "Prop poi's: 0.0\n",
      "n outliers in loan_advances 1\n",
      "Prop poi's: 1.0\n",
      "n outliers in from_messages 2\n",
      "Prop poi's: 0.0\n",
      "n outliers in other 2\n",
      "Prop poi's: 0.5\n",
      "n outliers in from_this_person_to_poi 4\n",
      "Prop poi's: 0.25\n",
      "n outliers in poi 0\n",
      "Prop poi's: nan\n",
      "n outliers in director_fees 9\n",
      "Prop poi's: 0.0\n",
      "n outliers in deferred_income 5\n",
      "Prop poi's: 0.6\n",
      "n outliers in long_term_incentive 2\n",
      "Prop poi's: 0.5\n",
      "n outliers in from_poi_to_this_person 2\n",
      "Prop poi's: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khaoz\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:5: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "C:\\Users\\khaoz\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:5: VisibleDeprecationWarning: using a boolean instead of an integer will result in an error in the future\n",
      "C:\\Users\\khaoz\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:6: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "C:\\Users\\khaoz\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:6: VisibleDeprecationWarning: using a boolean instead of an integer will result in an error in the future\n",
      "C:\\Users\\khaoz\\Anaconda2\\lib\\site-packages\\numpy\\core\\_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "C:\\Users\\khaoz\\Anaconda2\\lib\\site-packages\\numpy\\core\\_methods.py:70: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\khaoz\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:10: RuntimeWarning: invalid value encountered in rint\n"
     ]
    }
   ],
   "source": [
    "# identify outliers and compare POIs\n",
    "poi = np.array(df[\"poi\"])\n",
    "for feat in df.columns.values:\n",
    "    f = np.array(df[feat])\n",
    "    f[f==\"NaN\"] = 0\n",
    "    f[f==\"None\"] = 0\n",
    "    f = f.astype(float)\n",
    "    poi_out = poi[is_outlier(f)]\n",
    "    print \"n outliers in \" + feat, len(poi_out)\n",
    "    print \"Prop poi's:\", poi_out.mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are not very many outliers in general, and some of the more frequent ones (e.g. total_stock_value and bonus) contain a much more extreme proportion of POIs than data average (0.8 and 0.75 vs 0.12).\n",
    "\n",
    "Based on that, I'm reluctant to take any radical steps here, so I'll let the ourliers be for now.\n",
    "\n",
    "What I will do, however, is add a couple new features to the data. \n",
    "\n",
    "First, I noticed there are people who got really huge boni (bonuses?) from Enron, over $1 million. That seems suspicious to me, so I'll include this as an extra discreet feature (> or < 1 mil.) into the dataset. \n",
    "\n",
    "Second, as we have around 44% missing values, and some persons have much more missing information about them than others, I wanted to include the proportion of missing data per person as a feature. Maybe those POIs were more secretive somehow.\n",
    "\n",
    "So I'll include these two new features and compose a new feature list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add feature 1: discreet 1/0 for bonus over 1 mil\n",
    "huge_bonus = []\n",
    "for bonus in df[\"bonus\"]:\n",
    "    if bonus > 1000000 and bonus != \"NaN\":\n",
    "        huge_bonus.append(1)\n",
    "    else:\n",
    "        huge_bonus.append(0)\n",
    "df[\"huge_bonus\"] = huge_bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add feature 2: proportion NaNs for given person\n",
    "prop_nans = []\n",
    "rows = df.iterrows()\n",
    "for p in range(len(df)):\n",
    "    person = rows.next()[1]\n",
    "    prop_nans.append(len(person[person==\"NaN\"]) / float(len(person)))\n",
    "df[\"prop_nans\"] = prop_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poi',\n",
       " 'salary',\n",
       " 'to_messages',\n",
       " 'deferral_payments',\n",
       " 'total_payments',\n",
       " 'exercised_stock_options',\n",
       " 'bonus',\n",
       " 'restricted_stock',\n",
       " 'shared_receipt_with_poi',\n",
       " 'restricted_stock_deferred',\n",
       " 'total_stock_value',\n",
       " 'expenses',\n",
       " 'loan_advances',\n",
       " 'from_messages',\n",
       " 'other',\n",
       " 'from_this_person_to_poi',\n",
       " 'director_fees',\n",
       " 'deferred_income',\n",
       " 'long_term_incentive',\n",
       " 'from_poi_to_this_person',\n",
       " 'huge_bonus',\n",
       " 'prop_nans']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list = list(df.columns.values)\n",
    "features_list.remove(\"poi\")\n",
    "features_list = [\"poi\"] + features_list\n",
    "features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's format the data with these features and run a quick classifier comparison with all the popular classifiers, using the test function from tester.py. Just to get a first impression of how they perform here, before doing any feature selection. Note that we don't care about feature scaling here as I'm not using any clustering, which would be sensitive to that. \n",
    "\n",
    "Also note that the test function uses Stratified Shuffle Split for cross-validation. Cross-validation is important as it allows us to realistically assess our model's performance. If we would use our whole data set for both fitting our classifier and predicting the target classes, we would inevitably overestimate the model's performance - our model would overfit, i.e. adjust itself too well to the data we have. Then, when we'd try to predict labels for a new dataset, chances are the model wouldn't do too well. To avoid this problem, one approach would be to split our data into a training set and a test set, and use only the training set for model fitting, and only the test set for prediction. Cross-validation automates this concept. In case of the Stratified Shuffle Split, it divides the data into a specified number of subsets (called folds), shuffles their order, and then fits the model to the dataset comprised of all the folds but one, which is used for testing. This process is repeated with each fold serving as a testing fold once. At each step, model performance is assessed and at the end an average is produced.\n",
    "\n",
    "It is particularly important to use cross-validation in our case, as we have quite imbalanced target classes. This means that just one train/test split would probably not have the same proportion of labels in both the training and testing data, which would be a problem for model assessment. The Stratified Shuffle Split is a particularly good method to deal with imbalanced classes and hence it is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_dict = df.to_dict(orient=\"index\")\n",
    "\n",
    "data = featureFormat(my_dict, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.73540\tPrecision: 0.21944\tRecall: 0.38500\tF1: 0.27954\tF2: 0.33452\n",
      "\tTotal predictions: 15000\tTrue positives:  770\tFalse positives: 2739\tFalse negatives: 1230\tTrue negatives: 10261\n",
      "\n",
      "######################################\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.79093\tPrecision: 0.21020\tRecall: 0.20600\tF1: 0.20808\tF2: 0.20683\n",
      "\tTotal predictions: 15000\tTrue positives:  412\tFalse positives: 1548\tFalse negatives: 1588\tTrue negatives: 11452\n",
      "\n",
      "######################################\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.85460\tPrecision: 0.36472\tRecall: 0.12200\tF1: 0.18284\tF2: 0.14073\n",
      "\tTotal predictions: 15000\tTrue positives:  244\tFalse positives:  425\tFalse negatives: 1756\tTrue negatives: 12575\n",
      "\n",
      "######################################\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.84000\tPrecision: 0.36842\tRecall: 0.28000\tF1: 0.31818\tF2: 0.29412\n",
      "\tTotal predictions: 15000\tTrue positives:  560\tFalse positives:  960\tFalse negatives: 1440\tTrue negatives: 12040\n",
      "\n",
      "######################################\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from tester import test_classifier\n",
    "\n",
    "# run quick classifier comparison\n",
    "clf = GaussianNB()\n",
    "test_classifier(clf, my_dict, features_list)\n",
    "print \"######################################\"\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "test_classifier(clf, my_dict, features_list)\n",
    "print \"######################################\"\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "test_classifier(clf, my_dict, features_list)\n",
    "print \"######################################\"\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "test_classifier(clf, my_dict, features_list)\n",
    "print \"######################################\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh nice, it looks like AdaBoost is actually not far off the mark for our goal of minimum 0.3 for both precision and recall. It actually has ~0.37 precision, but not enough recall at 0.28. Random Forest has comparable precision, but much worse recall, and Naive Bayes actually does surprisingly well on recall, but lacks in precision.\n",
    "\n",
    "All in all, AdaBoost still looks most promising to me. Maybe a grid search will be able to tune it up further?\n",
    "\n",
    "The idea behind the grid search is simple: instead of manually testing different parameter combinations for the model, a function will do this automatically. The inputs are the ranges of parameters to try, and the function goes over all possible combinations of parameters within these ranges (i.e. searching the grid). The performance is assessed using cross-validation and measured by a specified scoring function. In the end, the classifier with optimized parameters for the specific metric is obtained.\n",
    "\n",
    "Since the tester uses Stratified Shuffle Split, I'll use it as well for cross-validation in my grid search, with 100 splits. It's crucial to use cross-validation for the reasons I described earlier.  I'll also use \"f1\" for scoring, as we're interested in both precision and recall, and search over a small range of two main AdaBoost parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khaoz\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1 score: 0.260904761905\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.4000000000000004, n_estimators=49,\n",
      "          random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# run grid search on AdaBoost\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=100, random_state=42)\n",
    "params = {'n_estimators': range(48, 52),\n",
    "          'learning_rate': np.arange(1., 2., 0.1)}\n",
    "\n",
    "ada_clf = GridSearchCV(AdaBoostClassifier(random_state=42), params, cv=sss, scoring=\"f1\")\n",
    "ada_clf.fit(features, labels)\n",
    "print \"Best f1 score:\", ada_clf.best_score_\n",
    "print ada_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.4000000000000004, n_estimators=49,\n",
      "          random_state=42)\n",
      "\tAccuracy: 0.83373\tPrecision: 0.33105\tRecall: 0.24200\tF1: 0.27961\tF2: 0.25576\n",
      "\tTotal predictions: 15000\tTrue positives:  484\tFalse positives:  978\tFalse negatives: 1516\tTrue negatives: 12022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the best AdaBoost classifier\n",
    "test_classifier(ada_clf.best_estimator_, my_dict, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and funny enough, the tuned-up AdaBoost performs worse than before the grid search! With 49 estimators and a learning rate of 1.4 it is only able to produce 0.33 precision and 0.24 recall with the tester function. \n",
    "\n",
    "We'll have to do better than that! Let's try some feature selection and see if it helps.\n",
    "\n",
    "First of all, let's see if the two features I added are actually helping. I'll run the standard AdaBoost without them and see if the performance drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.84627\tPrecision: 0.39868\tRecall: 0.30100\tF1: 0.34302\tF2: 0.31651\n",
      "\tTotal predictions: 15000\tTrue positives:  602\tFalse positives:  908\tFalse negatives: 1398\tTrue negatives: 12092\n",
      "\n",
      "######################################\n"
     ]
    }
   ],
   "source": [
    "# test without my engineered features\n",
    "features_list_orig = list(features_list)\n",
    "features_list_orig.remove(\"huge_bonus\")\n",
    "features_list_orig.remove(\"prop_nans\")\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "test_classifier(clf, my_dict, features_list_orig)\n",
    "print \"######################################\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, the performance actually went up! We're at ~0.4 precision and 0.3 recall. Those were not very helpful features it seems... But now I'm curious whether there are more features in the way here. This warrants a more systematic approach.\n",
    "\n",
    "I'll now implement 10 decision trees and use their aggregate feature importances to decide which features to keep. I'll start with the full set of features and test different thresholds of importance, comparing the resulting feature sets' performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 0.53,\n",
       " 'deferral_payments': 0.0,\n",
       " 'deferred_income': 0.0,\n",
       " 'director_fees': 1.13,\n",
       " 'exercised_stock_options': 2.32,\n",
       " 'expenses': 1.1,\n",
       " 'from_messages': 1.2,\n",
       " 'from_poi_to_this_person': 0.58,\n",
       " 'from_this_person_to_poi': 0.0,\n",
       " 'huge_bonus': 0.0,\n",
       " 'loan_advances': 0.65,\n",
       " 'long_term_incentive': 0.0,\n",
       " 'other': 0.56,\n",
       " 'prop_nans': 0.3,\n",
       " 'restricted_stock': 0.55,\n",
       " 'restricted_stock_deferred': 0.0,\n",
       " 'salary': 0.0,\n",
       " 'shared_receipt_with_poi': 1.09,\n",
       " 'to_messages': 0.0,\n",
       " 'total_payments': 0.0,\n",
       " 'total_stock_value': 0.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = featureFormat(my_dict, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# use DT feature importances for feature selection\n",
    "feat_imps = np.zeros(len(features_list)-1)\n",
    "clf_dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "for _ in range(10):\n",
    "    clf_dt.fit(features, labels)\n",
    "    feat_imps +=  clf_dt.feature_importances_\n",
    "\n",
    "feat_imps = feat_imps.tolist()\n",
    "feat_names_imps = {}\n",
    "features_list.remove(\"poi\")\n",
    "features_list.sort()\n",
    "\n",
    "for i in range(len(features_list)):\n",
    "    feat_names_imps[features_list[i]] = round(feat_imps[i], 2)\n",
    "\n",
    "feat_names_imps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll compare three crude thresholds: above 0, above 0.5 and above 1.0 feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poi', 'loan_advances', 'bonus', 'prop_nans', 'expenses', 'from_poi_to_this_person', 'exercised_stock_options', 'from_messages', 'other', 'shared_receipt_with_poi', 'restricted_stock', 'director_fees']\n",
      "['poi', 'loan_advances', 'bonus', 'expenses', 'from_poi_to_this_person', 'exercised_stock_options', 'from_messages', 'other', 'shared_receipt_with_poi', 'restricted_stock', 'director_fees']\n",
      "['poi', 'expenses', 'exercised_stock_options', 'from_messages', 'shared_receipt_with_poi', 'director_fees']\n"
     ]
    }
   ],
   "source": [
    "features_list0 = [\"poi\"]\n",
    "features_list05 = [\"poi\"]\n",
    "features_list1 = [\"poi\"]\n",
    "for key in feat_names_imps:\n",
    "    if feat_names_imps[key] > 0:\n",
    "        features_list0.append(key)\n",
    "    if feat_names_imps[key] >= 0.5:\n",
    "        features_list05.append(key)\n",
    "    if feat_names_imps[key] >= 1:\n",
    "        features_list1.append(key)\n",
    "print features_list0\n",
    "print features_list05\n",
    "print features_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.84513\tPrecision: 0.39382\tRecall: 0.29950\tF1: 0.34024\tF2: 0.31457\n",
      "\tTotal predictions: 15000\tTrue positives:  599\tFalse positives:  922\tFalse negatives: 1401\tTrue negatives: 12078\n",
      "\n",
      "######################################\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.85173\tPrecision: 0.42244\tRecall: 0.30500\tF1: 0.35424\tF2: 0.32296\n",
      "\tTotal predictions: 15000\tTrue positives:  610\tFalse positives:  834\tFalse negatives: 1390\tTrue negatives: 12166\n",
      "\n",
      "######################################\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.84971\tPrecision: 0.46987\tRecall: 0.40550\tF1: 0.43532\tF2: 0.41692\n",
      "\tTotal predictions: 14000\tTrue positives:  811\tFalse positives:  915\tFalse negatives: 1189\tTrue negatives: 11085\n",
      "\n",
      "######################################\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoostClassifier()\n",
    "test_classifier(clf, my_dict, features_list0)\n",
    "print \"######################################\"\n",
    "clf = AdaBoostClassifier()\n",
    "test_classifier(clf, my_dict, features_list05)\n",
    "print \"######################################\"\n",
    "clf = AdaBoostClassifier()\n",
    "test_classifier(clf, my_dict, features_list1)\n",
    "print \"######################################\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the comparison results for the three thresholds with the standard AdaBoost:\n",
    "\n",
    "Feature importance threshold | Precision | Recall\n",
    "-----------------------------|-----------|-------\n",
    "0.0                          |0.39382    |0.29950\n",
    "0.5                          |0.42244    |0.30500\n",
    "1.0                          |0.46987    |0.40550\n",
    "\n",
    "It's clear that we should use the smallest set of features with over 1.0 feature importance.\n",
    "This set only has 5 predictors, 3 from the financial data and 2 from the email data.\n",
    "\n",
    "I'll now run a quick comparison of the other three classifiers with this shortened set of features, just out of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.29436\tPrecision: 0.16118\tRecall: 0.93700\tF1: 0.27504\tF2: 0.47740\n",
      "\tTotal predictions: 14000\tTrue positives: 1874\tFalse positives: 9753\tFalse negatives:  126\tTrue negatives: 2247\n",
      "\n",
      "######################################\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.81229\tPrecision: 0.36204\tRecall: 0.41200\tF1: 0.38541\tF2: 0.40093\n",
      "\tTotal predictions: 14000\tTrue positives:  824\tFalse positives: 1452\tFalse negatives: 1176\tTrue negatives: 10548\n",
      "\n",
      "######################################\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.84314\tPrecision: 0.39013\tRecall: 0.17400\tF1: 0.24066\tF2: 0.19568\n",
      "\tTotal predictions: 14000\tTrue positives:  348\tFalse positives:  544\tFalse negatives: 1652\tTrue negatives: 11456\n",
      "\n",
      "######################################\n"
     ]
    }
   ],
   "source": [
    "# run quick classifier comparison on new data\n",
    "clf1 = GaussianNB()\n",
    "test_classifier(clf1, my_dict, features_list1)\n",
    "print \"######################################\"\n",
    "\n",
    "clf1 = DecisionTreeClassifier()\n",
    "test_classifier(clf1, my_dict, features_list1)\n",
    "print \"######################################\"\n",
    "\n",
    "clf1 = RandomForestClassifier()\n",
    "test_classifier(clf1, my_dict, features_list1)\n",
    "print \"######################################\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so here are the results. Our AdaBoost now sports ~0.47 precision and ~0.41 recall. The simple Decision Tree also got boosted with the same 0.41 recall, but a lower ~0.36 precision. Random forest has good precision, and Naive Bayes shows outstanding recall, but they both lack on the other metrics. Note that this performance impovement was achieved by using only the 5 best features of the 21 total. The joys of feature selection! The power of sparsity!\n",
    "\n",
    "Now I'll try the grid search for AdaBoost one more time, with the same parameter grid, just to see if any improvement happens this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1 score: 0.423047619048\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.7000000000000006, n_estimators=49,\n",
      "          random_state=42)\n"
     ]
    }
   ],
   "source": [
    "data = featureFormat(my_dict, features_list1, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# run second grid search on AdaBoost on new data with same params\n",
    "ada_clf.fit(features, labels)\n",
    "print \"Best f1 score:\", ada_clf.best_score_\n",
    "print ada_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.7000000000000006, n_estimators=49,\n",
      "          random_state=42)\n",
      "\tAccuracy: 0.83957\tPrecision: 0.43059\tRecall: 0.38150\tF1: 0.40456\tF2: 0.39040\n",
      "\tTotal predictions: 14000\tTrue positives:  763\tFalse positives: 1009\tFalse negatives: 1237\tTrue negatives: 10991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the best AdaBoost classifier once again\n",
    "test_classifier(ada_clf.best_estimator_, my_dict, features_list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, it got worse again. The grid search-produced best classifier with 49 estimators and 1.7 learning rate has around 0.03 worse precision and recall than the standard 50 estimators, 1.0 learning rate AdaBoost.\n",
    "\n",
    "One idea would be to try and tweak the grid search further with a custom scoring function. I have an impression that the f1-scoring still isn't optimal for our case. One could also try to search over a much larger parameter space.\n",
    "\n",
    "However, I feel that these nuances lie outside the scope of this project. I pronounce the standard AdaBoost the winner! After all, it achieved an impressive ~0.47 precision and ~0.41 recall in the tester function with Stratified Shuffle Split. \n",
    "\n",
    "In human-understandable language it means that the AdaBoostClassifier is able to identify 41% of real POIs, and 47% of persons it classifies as POIs are actually ones. And it only uses 5 features to achieve that: 'expenses', 'exercised_stock_options', 'from_messages', 'shared_receipt_with_poi', and 'director_fees'.\n",
    "\n",
    "\n",
    "## Impressive!\n",
    "(not really, but it'll do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export my classifier, data, and features\n",
    "my_dataset = my_dict\n",
    "features_list = features_list1\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
